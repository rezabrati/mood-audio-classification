{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_label(file_path):   \n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def load_data(file_path):   \n",
    "    with np.load(file_path, allow_pickle=True) as data:\n",
    "        list_audio_ids = data['audio_id']\n",
    "        list_features = data['features']\n",
    "\n",
    "    return list_audio_ids, list_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "labels_path = './data/label_59.csv'\n",
    "data_path = './data/vggish_final.npz'\n",
    "\n",
    "print('start preparing data ...')\n",
    "# load data and labels\n",
    "list_audio_ids, list_features = load_data(data_path)\n",
    "print(list_audio_ids.shape)\n",
    "dic_label = load_label(labels_path)\n",
    "dic_label = dic_label.set_index('audio_Id')\n",
    "labels = [dic_label.loc[int(audio_id)].values for audio_id in list_audio_ids]\n",
    "\n",
    "samples = list(zip(list_features, labels))\n",
    "print(f\"There are {len(samples)} samples in the dataset.\")\n",
    "num_items = len(samples)\n",
    "\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(samples, [num_train, num_val])\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AudioClassifier (nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=128, out_features=59)\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "audioModel = AudioClassifier()\n",
    "device = torch.device(\"cpu\")\n",
    "audioModel = audioModel.to(device)\n",
    "next(audioModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training\n",
    "def training(model, train_dl, num_epochs):\n",
    "\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.MultiLabelSoftMarginLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # Normalize the inputs\n",
    "        inputs = inputs.to(torch.float)\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1, 128, 1407)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        \n",
    "        #map outputs to range of 0-1\n",
    "        outputs = torch.sigmoid(outputs).cpu()     \n",
    "\n",
    "        prediction = outputs > 0.6\n",
    "\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "    \n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.3f}, Accuracy: {acc:.3f}')\n",
    "\n",
    "  print('Finished Training')\n",
    "  \n",
    "num_epochs=10   \n",
    "\n",
    "training(audioModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(audioModel.state_dict(), \"./code/models/cnn/cnn_h/audioModel_aug_59.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inference\n",
    "\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "\n",
    "\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "  all_labels = []\n",
    "  transformer_all_preds = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "      all_labels += labels.to(torch.float)\n",
    "      \n",
    "      # Normalize the inputs\n",
    "      inputs = inputs.to(torch.float)\n",
    "      inputs = inputs.reshape(inputs.shape[0], -1, 128, 1407)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      outputs = torch.sigmoid(outputs).cpu()      \n",
    "\n",
    "      \n",
    "      prediction = outputs > 0.5\n",
    "      transformer_all_preds.extend(prediction.tolist())\n",
    "\n",
    "\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "  \n",
    "  T_Y = np.array(transformer_all_preds)\n",
    "  Y =  np.array([a.detach().cpu().numpy() for a in all_labels])\n",
    "\n",
    "  tsrf_f1 = f1_score(Y,  T_Y, labels=None, pos_label=1, average='macro', sample_weight=None, zero_division='warn')\n",
    "  print(\"cnn f1 macro: \", tsrf_f1)\n",
    "  \n",
    "  tsrf_f1_micro = f1_score(Y,  T_Y , labels=None, pos_label=1, average='micro', sample_weight=None, zero_division='warn')\n",
    "  print(\"cnn f1 micro: \", tsrf_f1_micro)\n",
    "\n",
    "  average_precision_score_micro = average_precision_score(Y,  T_Y , average = \"micro\")\n",
    "  print(\"cnn average_precision_score_micro: \", average_precision_score_micro)\n",
    "\n",
    "  roc_auc_score_micro = roc_auc_score(Y, T_Y , average = \"micro\")\n",
    "  print(\"cnn roc_auc_score_micro: \", roc_auc_score_micro)\n",
    "\n",
    "  average_precision_score_macro = average_precision_score(Y,  T_Y , average = \"macro\")\n",
    "  print(\"cnn average_precision_score_macro: \", average_precision_score_macro)\n",
    "\n",
    "  roc_auc_score_macro = roc_auc_score(Y,  T_Y , average = \"macro\")\n",
    "  print(\"cnn roc_auc_score_macro: \", roc_auc_score_macro)\n",
    "\n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.3f}, Total items: {total_prediction}')\n",
    "\n",
    "\n",
    "model = AudioClassifier()\n",
    "model.load_state_dict(torch.load(\"./code/models/cnn/cnn_h/audioModel_aug_5.pth\"))\n",
    "model.eval()\n",
    "inference(model, val_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d8a55b634813c01aa2911a22593b18d6077248b2bb5120b73d8cde03914652e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
